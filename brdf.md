
DualPhys-GS: Dual Physically-Guided 3D Gaussian Splatting for Underwater Scene Reconstruction
ABSTRACT

在水下场景三维重建中，传统基于大气光学模型的方法无法有效应对水体介质特有的光波长选择性衰减和悬浮粒子散射效应，导致重建结果远距离处出现颜色失真、几何伪影及塌陷现象。本文提出DualPhys-GS框架，通过双路径优化机制实现高质量水下重建。我们提出了基于特征引导的衰减-散射双重建模机制：RGB引导的衰减优化模型结合RGB特征和深度信息处理边缘和结构细节；多尺度深度感知散射模型利用特征金字塔网络和注意力机制捕捉不同尺度的散射效应。同时，我们设计了特殊损失函数，边缘感知散射损失以保持结构边缘锐利度、多尺度特征损失捕获全局与局部结构信息、水体类型自适应损失动态调整损失权重以及衰减散射一致性损失确保物理一致性。此外，我们还设计了场景自适应机制，能够自动识别水体类型(如清澈的珊瑚礁水域或浑浊的沿海水域)，动态调整散射和衰减参数及优化策略。实验结果表明，我们的方法各种指标上优于现有方法，特别在悬浮物密集区域和远距离场景中重建质量显著提升。
INTRODUCTION
随着海洋资源开发与生态监测需求的增长，水下场景三维重建技术的重要性日益提高。但是，相较于大气环境，水下场景的可用数据规模受限于采集成本与设备条件，同时面临着独特的物理挑战：（1）波长选择性衰减：水体介质对不同波长光线的吸收率差异显著，尤其是长波光信号（如红光）随距离急剧衰减，导致色彩失真（典型的蓝绿色调）现象，影响渲染图像色彩真实性；（2）多重散射效应：水中悬浮粒子（如浮游生物、有机碎屑和矿物质）引发的各向异性散射效应。特别是在远距离或浑浊水体中，前向散射引发的几何结构模糊与背景噪声干扰更为显著，背景散射则导致图像"雾化"，进一步降低了特征匹配的准确性。（3）水体环境多样性：不同水体呈现出不同的的光学特性，导致固定参数模型难以在不同水体环境中保持鲁棒性。传统基于大气光学模型的重建方法（MVS）在水下环境中应用时，不仅在几何精度上出现系统性偏差，在纹理保真度上也呈现显著退化，严重制约了水下三维重建技术在海洋科学、水下机器人自主导航和虚拟水下旅游等领域的应用潜力。

近年来，Neural Radiance Fields（NeRFs）在水下场景中展现出独特优势。NeRFs通过神经隐式辐射场建模，结合体渲染技术能够有效应对水下场景的部分挑战。体渲染技术通过沿光线累计密度和颜色的方式，非常适合模拟水体中的半透明效果，可以更精确的表达不同深度的水下物体与悬浮粒子之间复杂的光学交互；另外，体渲染的体积积分特性能够模拟光线在不同水介质中的传播过程，更有助于波长依赖性光传输的建模。然而，基于NeRFs的方法其训练和渲染过程往往伴随着高昂的计算开销，无法实现实时渲染；而且在全局优化过程中体渲染难以精确区分波长选择性衰减的影响，导致远距离处色彩失真，这些限制制约了其在实时水下场景重建中的潜力。

最近，基于显式高斯表征的3D Gaussian Splatting（3DGS）在三位场景重建领域取得了突破性进展。3DGS创新性地引入显式三维高斯表征与可微分光栅化管线，实现实时渲染效率的同时保持了视觉质量。另外，其各向异性的高斯基元可以精确建模场景的几何细节，有效保留水下场景的边缘结构；其基于透明度的alpha混合机制更容易整合基于物理的色彩恢复模型。然而，3DGS基于大气介质的光传输假设，直接迁移至水下场景时面临根本性局限：忽略了水体特有的光学效应，水体对光线的波长依赖性衰减导致远处物体呈现系统性的色偏（红色信息大幅丢失）；悬浮粒子引发的各向异性散射效应干扰了深度一致性估计，导致密度场与几何结构重建偏差。这些问题在3DGS重建结果中表现为远距离处海底地形的几何伪影、珊瑚礁纹理的系统性色偏以及场景表面假体积效应（pseudo-volumetric effect）。
为解决这些问题，本文提出一种基于3DGS的双路径优化框架DualPhys-GS，一种基于双重物理建模的场景自适应方法实现高质量的水下场景重建。DualPhys-GS的核心创新在于设计了基于特征引导的衰减-散射双重建模机制，将水下光学传播过程精确分解为衰减和散射两个关键物理过程。对于散射过程，我们提出多尺度深度感知散射模型，通过特征金字塔网络和注意力机制捕捉不同尺度的散射效应；对于衰减过程，我们设计RGB引导的衰减优化模型，结合RGB特征和深度信息精确处理场景边缘与结构细节，并引入波长物理先验模拟水体对不同波长光的差异化吸收。为确保重建结果符合水下光学规律，我们设计了边缘感知散射损失、多尺度特征损失和衰减散射一致性损失等特殊损失函数，共同保障了系统在各类水下环境中的高质量重建能力。此外，针对水下环境的多样性，我们提出场景自适应机制，能够自动识别水体类型并动态调整优化参数与策略。具体的贡献如下：
提出了基于特征引导的衰减-散射双重建模机制，通过多尺度深度感知散射模型和RGB引导的衰减优化模型，实现了水下光学传播过程的精确模拟；
设计了边缘感知散射损失、多尺度特征损失、衰减散射一致性损失、水体类型自适应损失等，确保模型输出符合水下光学物理规律；
提出了场景自适应机制，能够自动识别水体类型并动态调整优化策略，在不同水下场景中可以保证高质量重建；
RELATED WORKS
A.基于传统多视角几何的水下重建
传统水下三维重建主要依赖多视角立体视觉（Multi-View Stereo, MVS）技术[1]。针对水体介质的光学干扰，研究者提出基于物理模型的补偿方法：Chambah等[2]将Jaffe-McGlamery光传输方程应用于水下场景，通过背景散射模型估计背景散射光，并结合颜色校正恢复目标反射率，但其高度依赖理想化水体假设，在参数变化显著的实际环境中表现不稳定；Drews-Jr等[3]利用暗通道先验（Dark Channel Prior）估计透射率场，有效抑制前向散射引起的模糊效应。然而，此类方法依赖精确的水体光学参数标定（如衰减系数、散射相位函数），在浑浊水域中鲁棒性不足。近年来，深度学习方法被引入该领域，Li等[4]设计双分支网络分别估计介质参数与场景深度，减轻了对物理先验参数的依赖，但受限于监督数据的稀缺性，同时由于水体类型的多样性使模型难以泛化至不同光学特性的水下环境。

B.基于NeRF的水下重建
神经辐射场（NeRF）通过隐式神经表征实现高质量三维重建，其强大的隐式表征能力与体渲染机制为水下场景复杂的光学现象提供了新的思路，大致分为增强物理模型和光学效应解耦两个方向，但其在水下场景的应用仍面临介质散射的严峻挑战。在增强物理模型方向中，WaterNeRF[8]首次将Beer-Lambert衰减定律与体积渲染方程结合，成功模拟了水下光线的波长依赖性衰减现象，该方法引入了严格的物理约束，显著提升了远距离场景的色彩保真度，但对散射效应的处理不足，无法有效解决悬浮粒子引起的背景辐射干扰。Levy等人[11]提出的Seathru-NeRF首次显式建模散射介质物理特性，通过分离直接传输光与散射光路径，提高了浑浊介质中的重建质量，但其仍依赖全局均匀散射系数假设，忽略了水体介质参数的空间变化特性，在复杂水体中存在明显局限性。
光学效应解耦体现在Ye等人[6]提出水下光场保留方法和WaterHE-NeRF[9]中，前者通过联合优化辐射场与光传输路径隐式建模前向散射，但其相位函数仅支持各向同性近似，难以描述浑浊水域中悬浮粒子的各向异性特征。后者将水体折射、表面波动与介质散射解耦为独立隐式场，但其计算复杂度严重限制了渲染效率，难以满足实时性应用需求。最新研究Beyond NeRF Underwater[10]通过可微分光线追踪联合优化目标反射率与介质参数，进一步提升了渲染精度，但其依赖深度传感器数据，难以适用于单目或稀疏视图场景。
现有方法普遍存在背景辐射场与目标反射率的耦合优化导致远距离重建色偏，同时散射相位函数的低阶近似无法准确描述沙粒-浮游生物混合介质的各向异性特征，这限制了在复杂水下环境中的鲁棒性。

C.基于3DGS的水下重建
3D Gaussian Splatting（3DGS）凭借显式高斯表征与微分光栅化优势，为水下实时重建提供了新范式。WaterSplatting[12]首次将3DGS应用于水下场景，通过可学习衰减系数实现波长选择性吸收补偿，并结合体渲染技术沿光线计算密度和颜色，有效缓解远距离色彩失真问题，还保留了3DGS的高效渲染特性。但其简化的物理模型完全忽略了散射效应，导致在浑浊水体中边缘结构模糊、细节丢失。SeaSplat[13]提出了更完整的物理成像模型，将Henyey-Greenstein相位函数嵌入高斯属性优化以支持单次散射近似，使其能够模拟水中悬浮粒子的散射效应，显著提升了浑浊环境中的重建质量。但其单一尺度模型无法同时兼顾近景细节和远距离处的结构特性，对多重散射和背景光缺乏显式建，并且全局控制散射残差项，缺乏场景自适应能力。在水体类型多样的实际应用中，这种全局固定的参数模型难以同时适应从清澈珊瑚礁到浑浊沿海的不同水下环境。 Aquatic-GS[15]通过混合隐式-显式表征预测空间变化的水体介质参数，增强了对复杂水体光学特性的适应能力。其隐式组件能够捕捉更精细的水体参数变化，但该组件会导致渲染速度显著下降，牺牲了3DGS的实时渲染优势。最新工作Gaussian Splashing[16]直接对水体体积进行高斯化建模以支持动态效应，为水波等动态现象提供了新的表达方式。然而，该方法主要关注表面动态效应，未解决静态场景的几何精度问题。
上述方法虽然尝试从不同角度解决上面提到的水下场景重建面临的核心问题，但仍然存在根本性局限：（1）衰减-散射参数耦合：水下光学传播涉及两种物理过程，波长选择性衰减和多重散射效应，但是之前的方法[ ]通常将二者在同一优化框架中处理，导致参数相互干扰。当悬浮粒子密集时，散射效应占主导，框架倾向于高估衰减系数；在清澈水域中，衰减效应占主导，就会低估散射参数。这种参数耦合会导致远距离处物体呈现系统性色彩偏差或几何塌陷；（2）水体类型适应性局限：从珊瑚礁的清澈水域到沿海的浑浊环境，水体光学特性差异显著。现有方法多采用全局固定参数模型，缺乏自动识别水质类型并动态调整参数的机制。这使得在同一参数设置下，系统无法同时适应不同水体环境，导致非目标水域性能显著下降。当从训练环境迁移到新水域时，通常需要人工重新调整参数，严重限制了实际应用场景；（3）单一尺度表征局限：框架难以同时准确处理近景细节和远景整体特性，近距离物体散射特性与远处背景存在显著差异，单一尺度难以有效捕捉多尺度光学效应，导致近景过度模糊或者远景细节丢失。

PRELIMINARIES
A.3D Gaussian Splatting(3DGS)
3D高斯泼溅（3D Gaussian Splatting，3DGS） 是一种显式的三维重建场景表示方法，通过将点云膨胀为各向异性高斯基元（Gaussian primitives）来建模三维场景。每个高斯基元Gi由中心位置、协方差矩阵以及不透明度αi和视角的颜色属性组成。高斯基元的密度分布函数定义如下:

其中∈R3表示椭球中心的空间坐标，协方差矩阵i=RiSi2RTi表示高斯椭球的形状和方向。3D高斯沿任意视角投影到2D平面上，投影后的2D高斯保持高斯分布特性，其在图像平面上的协方差矩阵可通过雅可比矩阵 J计算，满足实时光栅化需求。再通过视点深度排序高斯基元执行alpha混合实现半透明效果。给定图像平面上的像素坐标x，其最终颜色C(x)计算如下：

其中颜色项ci由球谐函数SH编码，它和αi表示第i个高斯的颜色和密度。

B.水下物理成像模型
相比于大气介质，对水下场景进行重建时需要考虑水作为介质产生的影响。光线在水下传播时会经历衰减效应和反向散射效应的相互作用，从而导致光在水下传播过程中被吸收，最终导致渲染后的图像出现颜色不自然和伪影。经典的水下图像形成可表示为直接透射光和背景散射光的线性叠加，其表达式如下：

其中I为像素x出传感器接收的辐照度，J表示为无水衰减时的颜色，TD=是直接透射率，B∞表示无限远距离处的背景，表示背景透射率，与深度信息z相关。
METHOD
A.DualPhys-GS流程图

Figure 1. 我们的DualPhys-GS方法的流程图。我们利用物理双路径模型来约束3D高斯泼溅[7]进行水下场景重建，这要求我们的框架能够对水下场景的原始颜色J与水介质影响进行解耦建模。首先，输入图像通过运动恢复结构得到初始点云和相机位姿，然后经由水体类型分类器识别当前水域特性，自动配置场景自适应机制。在衰减路径中，RGB特征提取网络辅助衰减模型精确捕获波长选择性衰减效应；在散射路径中，特征金字塔网络支持散射模型建模多尺度各向异性散射现象。两路径生成的衰减图和散射图通过物理成像模型合成最终渲染结果。在训练过程中，我们应用多重损失约束：边缘感知损失Ledge保持结构清晰度，一致性损失Lconsistency确保衰减与散射的物理相关性，多尺度特征损失Lms捕获不同尺度的结构信息，以及水体自适应损失Ladaptive动态调整损失权重。这种多重约束机制使DualPhys-GS能够学习底层3D高斯表示与水下成像参数的最优组合，实现高质量水下场景重建。

B.基于特征引导的衰减-散射双重建模机制
水下环境的光学特性远比大气环境复杂，主要源于水作为介质对光线传播的影响。当光线穿过水体时，会受到两种主要物理现象的影响：光线衰减和反向散射。光线衰减会导致远距离处物体的颜色失真，反向散射会在图像中引入“雾状”效果，导致重建结果差。传统的水下物理成像模型比较简单，将水下图像分解为衰减直接光和散射光的组合。然而，这种简化处理无法有效应对水下场景的复杂性，尤其是在处理边缘、深度变化区域以及不同水体条件时表现不佳。为解决这个问题，我们提出了基于特征引导的衰减-散射双重建模机制，将水下光学传播过程精确分解为衰减和散射两个关键物理过程，并分别通过特征增强方式进行建模。我们提出的水下图像形成模型可表示为：

其中I是观察后的水下图像，J是真实图像，A是衰减图，B是散射项。这种双重建模机制通过解耦衰减和散射的优化过程，避免了参数相互干扰，同时引入了多种特征引导策略增强模型表达能力，RGB图像引导衰减优化，深度信息引导散射优化，能够精确模拟水下环境中的光学现象，显著提升重建质量和物理准确性。

衰减模块。在水下场景重建中，波长选择性衰减是目前面临的巨大挑战。当光线在水体中传播时，不同波长的光被吸收速率存在显著差异：红光（长波长）首先被吸收，导致远处物体呈现典型的蓝绿色调；而蓝光（短波长）穿透能力最强，在远距离仍能保持相对强度。针对这个问题，我们设计了基于RGB引导的衰减模型，在深度信息的基础上，又利用RGB图像提供的边缘信息来指导衰减过程，加入边缘感知机制处理深度图不连续性导致的边界伪影问题。我们的衰减模型结合了深度信息、RGB特征和边缘感知，可表示为：

其中，是波长相关权重向量，是基础衰减系数，E(IRGB,D)是边缘感知因子，γ是边缘调制强度常数。基于RGB引导的衰减模型先通过设计的RGB提取网络从输入图像中提取颜色分布和纹理信息，使衰减模型能够识别场景中因波长选择性衰减导致的颜色失真区域。这些RGB信息和深度特征结合后，模型可以更准确地估计每个区域应用的真实颜色。
另外，我们的衰减模型还引入了波长物理先验，通过可学习的通道特定权重来显式建模不同颜色通道（红、绿、蓝）的衰减差异。光线在水中进行传播时，红光衰减最快，蓝光衰减最慢。波长物理先验可以使衰减模型生成更符合物理规律的渲染图像结果。与传统仅基于深度的衰减模型相比，基于RGB引导的衰减模型在远距离处物体的色彩还原质量上有显著提升，尤其是在红色通道信息的恢复上，有效解决了远处物体呈现不自然蓝绿色调的问题。

散射模块。光线在均质水体中传播时，散射强度随深度呈指数增加，可以简单的表示为：

其中，B表示散射图，B∞表示无限远处的背景颜色，β表示散射系数，d表示深度。
	水下悬浮粒子引发的散射效应是导致重建质量下降的重要原因，它们对入射光产生复杂的角度依赖性散射。另外，与均质大气环境不同，水体散射呈现高度各向异性特性，近景和远景物体的散射表现不同，单一尺度模型难以同时准确处理近处细节和远处整体特性。

我们提出了多尺度散射模型，引入局部结构特征，通过特征金字塔网络捕捉不同尺度的深度信息，包含处理原始、1/2、1/4分辨率的三个特征提取分支，多尺度特征计算公式为：

其中表示下采样n倍，表示上采样n倍，fi 表示不同尺度的特征提取函数，fatt表示注意力增强模块。这种设计能够同时关注局部细节和全局结构。为了增强特征提取的能力，我们在多尺度散射模型中引入了通道和空间双重注意力机制。通道注意力通过自适应权重突出关键特征通道，而空间注意力则关注图像中散射变化显著的区域，提升模型对不同场景不同深度和水体条件的适应能力。近景区域依赖高分辨率分支捕获精细边缘和纹理细节，而远景区域依赖低分辨率分支把握整体散射趋势。我们提出的多尺度散射模型计算公式可表示为：

其中βb(D)是基础散射系数，C(D)是深度置信度因子，Ed(D)是深度边缘因子，M(D)多尺度特征权重，λ和δ是调制因子。此外，我们还设计了深度置信度评估模块，根据深度估计可靠性动态调整散射计算，并通过边缘感知处理机制缓解深度不连续导致的边界伪影问题，从而实现对水下散射现象的精确模拟。

C.Loss Function
除了基本的损失函数外，我们设计了边缘感知散射损失、多尺度特征损失、水体类型自适应损失和衰减散射一致性损失，以确保我们的衰减和散射模型能够在多种复杂条件下实现高质量的重建。

在水下物理成像模型中，衰减与散射都表现出与深度相关的特定物理规律，随着深度增加，水介质散射增强而直接透射光衰减加剧。然而，之前的方法通常将散射与衰减作为独立参数优化，缺乏物理约束机制，导致近处物体表现处高散射地衰减或远距离处出现低散射高衰减的非物理现象。这种不一致性严重影响了视觉真实感，所以我们提出了衰减散射一致性损失：

其中，E表示期望值，Bs表示散射分量，D表示深度图，A表示衰减分量，T表示透射率图，是水下场景中原始光线成功穿过水体介质到达相机的比例。该约束通过三个核心组件：（1）散射项Bs和深度D的正相关性约束；（2）衰减项A与深度D的负相关性约束；（3）散射与透射率T互补性约束。通过这三个组件的一致性约束确保水下场景的渲染图象符合物理一致性。

不同的水体类型呈现出不同的光学特性，在浑浊水体中散射效应更加显著，而在清澈水体中衰减效应更加突出。我们设计了水体类型自适应损失函数，通过分析图像的颜色分布特征（主要是蓝绿比）来确定水体类型指标，进而动态调整损失函数各部分的权重。该表达式可表示为：

其中，权重系数α(w)和β(w)由水体类型w动态确定。我们通过将图像的蓝绿比映射为一个标准化的水体类型指标w来估计水体类型，在清澈水体中衰减损失权重增大，而在浑浊水体中散射损失权重增大，从而使训练过程自适应于不同的水体环境。

在水下场景中，场景边缘往往对应着深度不连续的区域，这些区域的散射特性与深度连续区域存在显著差异，使得散射模型无法准确处理边缘区域的散射不连续性，这导致边缘模糊或结构扭曲，严重影响视觉真实感和几何准确性，尤其在复杂结构如珊瑚礁场景中更为明显。因此我们设计了深度感知边缘损失来保持约束：

其中，Wedge为深度边缘权重，根据局部深度梯度大小动态分配。平滑约束权重保持结构边缘的锐利度。该损失在深度变化剧烈的边缘区域施加更强的约束，迫使散射模型学习边缘处的精确散射特性而非简单平滑。该损失函数特别关注物体轮廓与背景的交界处，精确建模前景物体与背景水体之间的散射过渡，有效缓解了常见的边缘模糊或结构扭曲问题，提高了水下场景重建的边缘保真度。

由于单一尺度难以同时捕捉图像的高频细节和低频结构，所以我们提出了多尺度特征损失函数，通过在多个空间尺度上比较输出与目标来捕获全局和局部结构信息，该表达式如下：

其中，O是渲染图像，T是真实图像，S为下采样的一组尺度因子，包括原始分辨率、1/2、1/4，λms为权重系数，Ds为对下采样的输入和渲染图像进行操作。L1(O,T)表示在原始分辨率下计算的损失，后半部分表示在1/2、1/4分辨率下计算损失。最后通过结合原始尺度和多个下采样尺度的损失实现在不同层次的结构细节上保持一致性。

最后我们的总损失函数为：

D.场景自适应机制

我们提出了一套完整的场景自适应机制，通过自动识别不同水体类型并动态调整优化策略，使DualPhys-GS能够适应不同的水下场景。该机制由三个关键组件构成：水体类型分类器、场景特定参数库和自适应优化控制器。

不同水体具有显著不同的光学特性，若采用固定单一的参数模型，难以适应多样化的水下环境。因此，我们开发了水体类型分类器，根据输入RGB图像特征自动识别水体类型（清澈、中等、浑浊），表达式如下：

IRGB为输入的RGB图像，该分类器可以实时评估水下环境的光学特性，实现对不同水下环境的自适应处理。针对不同类型水下场景，我们定义了最优的光学参数，每个场景包含三个关键参数：衰减系数、散射系数、远距离背景颜色。通过分析输入的RGB图像特征来匹配对应的水体类型，然后应用对应相关参数。通过这两个组件，我们可以自适应不同的水下场景，解决了因为固定单一参数面对不同水下场景时的局限性。

虽然我们初始化了不同水下场景光学参数，但是不同水体环境仍需要不同的优化策略才能达到最佳效果。清澈水体中细微的衰减变化会对最终结果产生巨大影响，需要更精细的参数调整；浑浊水体中散射效应占主导地位，需要更强的散射约束。所以我们设计了自适应优化控制器，根据识别到的水体类型动态调整训练策略。控制器主要执行两个方面：（1）学习率调整：对于清澈水体环境，自动降低学习率，获得更精细的参数优化；（2）损失函数权重调整：通过水体类型自适应损失函数，根据水体清澈度来动态分配散射和衰减损失的权重，在浑浊水体中增加散射约束的权重；在清澈水体中，注重衰减约束。通过上面的三个关键组件相互作用，我们实现了在多样化水下场景中的自适应功能。

EXPERIMENTS
Datasets: 为了验证我们的方法，我们采用了SeaThru-NeRF发布的多视角水下场景数据集和salt pond数据集。SeaThru-NeRF包括从不同海域采集的四个场景，包括JapaneseGradens、IUI3、Curasao、Panama。

评估标准：我们选择了三种广泛使用的图像评估指标来验证新视角合成效果，通过峰值信噪比（PSNR）、结构相似性指数（SSIM）以及感知图像块相似性（LPIPS）[]，将最终得到渲染图像与真实图像进行比较，以此来评估视觉保真度。

实现细节：在训练开始前，我们利用COLMAP得到初始化点云并估计相机位姿。我们的训练策略和seasplat大体是一致的，所有场景的训练迭代次数为30000次，在10000次迭代开启水下颜色渲染。在初始化阶段，针对不同水下环境，我们设计了场景自适应机制，自动识别五种主要水体类型（Curasao、JapaneseGradens-RedSea、IUI3-RedSea、Panama和saltpond），并针对每种水体类型配置了特定的物理参数。训练过程中，我们采用Adam优化器分别优化高斯表征参数、散射模型和衰减模型，学习率分别设为5e-4和1e-4，针对不同水体环境，水体自适应机制会根据场景特性自动调整学习率。对于损失函数，我们动态调整不同损失项的权重，根据检测到的水体类型增强对应的物理约束，如在浑浊水体中增加散射损失权重，在清澈水体中强化衰减约束。本文所有实验均在单台工作站上完成，配置为Intel Core i7-14700KF处理器、48GB DDR5内存以及NVIDIA GeForce RTX 4090显卡（24GB显存）。
	Japanese Gardens	Panama	IUI3	Saltpond	Curasao
	PSNR↑	SSIM↑	LPIPS↓	PSNR↑	SSIM↑	LPIPS↓	PSNR↑	SSIM↑	LPIPS↓	PSNR↑	SSIM↑	LPIPS↓	PSNR↑	SSIM↑	LPIPS↓
STN[11]	21.74	0.77	0.29	26.01	0.79	0.32	15.62	0.40	0.63	11.93	0.51	0.58	30.08	0.87	0.19
3DGS[7]	21.47	0.85	0.22	29.64	0.90	0.17	26.89	0.83	0.19	27.10	0.75	0.29	28.01	0.88	0.21
SeaSplat[13]	22.70	0.87	0.18	28.76	0.9	0.15	26.67	0.87	0.21	27.47	0.75	0.25	30.30	0.90	0.19
ours	22.77	0.87	0.18	29.9	0.91	0.14	27.86	0.87	0.23	28.03	0.77	0.25	30.56	0.90	0.2
Table 1. Quantitative comparisons.在SeaThru-NeRF数据集上对所提方法进行评估的定性结果。“↑”表示数值越大越好，而“↓”则相反（数值越小越好）。红色标注的值表示最佳结果，绿色标注的值代表第二好的结果 。

Figure 2. 基于水下介质的新颖合成方法比较。Dualphys-GS表现出高质量的渲染结果。

Result and Discussion..

Quantitative results.
表1显示了在SeaThru-NeRF中不同数据集上的评估结果。从整体来看，我们的DualPhys-GS方法在大多数场景中表现出优越性能。与传统3DGS相比，我们的方法在PSNR指标上平均提升了1.14dB。相较于SeaThru-NeRF方法，DualPhys-GS的提升更为显著，PSNR平均提高了4.01dB。在结构相似度(SSIM)方面，我们的方法在所测试场景中均达到或超越了现有方法，平均SSIM达到0.86，相比SeaThru-NeRF的0.67提升了28.4%。即使与性能较好的SeaSplat相比，DualPhys-GS仍在Panama和IUI3-RedSea场景中实现了明显改进。这表明我们的双路径优化框架能够更好地保持水下场景的结构细节和边缘清晰度。另外，DualPhys-GS在感知质量评估(LPIPS)上也取得了平衡表现，特别是在含有复杂珊瑚结构和悬浮颗粒的Panama场景中，LPIPS达到0.14，优于其他所有方法。这证明了我们的RGB引导衰减优化模型和多尺度散射模型在处理水下光线衰减方面的有效性。虽然在个别场景（如IUI3的LPIPS指标）上略低于其他方法，但DualPhys-GS在整体视觉质量和物理准确性上仍然展现出显著优势。

Qualitative results.

从视觉重建质量来看，图2的渲染结果对比揭示了不同方法的特性差异。我们提出的DualPhys-GS方法重建质量优异主要归功于其创新的双路径优化机制。在巴拿马珊瑚礁等具有复杂几何结构的场景中，传统3DGS易产生高斯分布不稳定问题，而我们的方法通过RGB引导的衰减模型准确恢复了远距离处的色彩信息，特别是长波长的红色通道，有效解决了远处物体呈现不自然蓝绿色调的问题。同时，多尺度散射模型能够精确捕捉不同尺度的散射效应，使模型在水体环境中产生平滑连续的深度估计，同时保留细节特征。此外，水体类型自适应机制使模型能够根据不同水体环境自动调整优化策略，在不同水体条件下仍保持优异表现，展现出对复杂水下环境的适应能力。

    SeaThru-NeRF Dataset
Metrics		PSNR	SSIM	LPIPS
FULL MODEL(Dualphys-GS)		27.63	0.89	0.19

Model	RGBAttenuate only	26.86	0.88	0.19
	MultiScaleBackscatter only	27.07	0.89	0.19
	Water Type Adaptive only	27.20	0.89	0.18
	RGBAttenuate+MultiScaleBackscatter	27.27	0.89	0.18
	RGBAttenuate+MultiScaleBackscatter+Water Type Adaptive	27.29	0.89	0.18
Loss	Ladaptive only	27.08	0.88	0.18
	Lms only	27.14	0.89	0.19
	Lconsistency only	27.39	0.89	0.19
	Lconsistency+Ladaptive	27.40	0.89	0.18
	Lconsistency+Ladaptive+Lms	27.57	0.89	0.19
	Lconsistency+w/o Water Type Adaptive	27.47	0.89	0.18
Table 2. Model Ablations. 基于RGB引导的衰射模型、多尺度散射模型和水体场景自适应模块以及损失函数

Ablation Study.
我们使用定量结果（Tab. 2）来验证我们提出的DualPhys-GS中各个组件的有效性，包括基于特征引导的衰减散射模型、水体场景自适应模块和损失函数。基于SeaThru-NeRF数据集，我们通过测试该数据集下四个场景PSNR、SSIM和LPIPS的平均指标来评估模型各个模块和损失函数的重要性。

Model. 如表2所示，我们验证了基于特征引导的衰减散射双重建模机制的有效性。完整模型（DualPhys-GS）在PSNR平均指标上达到27.63dB，而仅使用基于RGB引导的衰减模型时PSNR下降至26.86dB，仅使用多尺度深度感知散射模型时PSNR为27.07dB。这些结果明确表明，我们提出的双重建模机制相比单一模型或者简化的衰减散射模型具有显著优势。可以看出，基于RGB引导的衰减优化模型结合RGB特征和深度信息，能有效处理波长选择性衰减现象，解决远距离物体的颜色失真问题；而多尺度散射模型通过特征金字塔网络捕捉不同尺度的散射效应，提升了重建质量。此外，水体类型自适应机制也起到了重要作用，当只开启水体场景自适应模块时，PSNR平均达到了27.20dB，能够根据不同水体特性动态调整优化策略。当将RGB衰减与多尺度散射结合时（PSNR 27.27dB），性能进一步提升，而完整模型整合所有组件后达到最佳效果，证明了各模块间的协同效应对提升水下场景重建质量至关重要。

Loss Function. 我们的消融实验也验证了各损失函数的重要性。最显著的是衰减散射一致性损失（Lconsistency）的关键作用，单独使用该损失函数时平均PSNR达到27.39dB，远高于仅使用场景自适应损失（27.08dB）和多尺度特征损失（27.14dB）的情况。这一结果表明，确保散射与深度正相关性、衰减与深度负相关性以及散射与透射率互补性的物理约束对于水下场景重建至关重要。当衰减散射一致性损失与场景自适应损失结合时（PSNR 27.40dB），模型能够同时保证物理一致性并能适应多种不同水下环境；进一步加入多尺度特征损失后（PSNR 27.57dB），模型能够在不同空间尺度上捕获全局和局部结构信息，显著提升重建质量。最后，值得注意的是，即使整合了三种损失函数，如果没有水体类型自适应机制，重建质量仍会有所下降（PSNR 27.47dB），这进一步证明了我们提出的场景自适应机制对于处理不同水体环境的重要性。
LIMITATION
在将DualPhys-GS部署于实际水下应用场景前，该方法仍需适应实时环境中的高效运行需求，特别是在动态构建3D高斯表示时的计算优化。这对于水下自主导航和珊瑚礁等特殊区域的自适应采样具有潜在应用价值。虽然我们的框架通过双路径优化机制精确模拟了水体衰减和散射效应，但在光学建模的完整性方面仍有局限：未考虑水面折射产生的焦散现象和水下设备（如潜水员或摄影装置）对场景照明的影响，这些在一些数据集（如Salt Pond）中较为明显。此外，真实海底环境充满动态元素（如随水流摆动的海草或游动的鱼群），而当前DualPhys-GS主要针对静态水下场景进行优化设计，对动态的水下场景或物体的建模能力有待进一步增强。

CONCLUSION
本文针对水下场景三维重建中的颜色失真与几何伪影问题，提出了一种基于3DGS的双路径优化框架Dualphys-GS。通过基于特征引导的衰减-散射双重建模机制，我们实现了水下光学传播过程的精确模拟。其中RGB引导的衰减优化模型结合RGB特征和深度信息精确处理场景边界和结构细节；而多尺度深度感知散射模型通过特征金字塔网络和注意力机制捕捉不同尺度的光学效应；然后，我们设计的一系列损失函数(边缘感知散射损失、多尺度特征损失、衰减散射一致性损失等)确保模型输出符合水下光学物理规律。另外，我们设计的场景自适应机制能够根据图像特征自动识别水体类型并相应调整参数，适应从清澈到浑浊的不同水下环境。实验结果表明，Dualphys-GS在复杂水下场景中显著提升了重建的几何精度与纹理保真度，尤其在悬浮物密集区域和远距离海底地形上表现优异。
REFERENCES
[1]Furukawa Y, Hernández C. Multi-view stereo: A tutorial. Foundations and Trends in Computer Graphics and Vision, 2015, 9(1-2): 1-148.
[2]Chambah M, et al. Underwater color constancy: Enhancement of automatic live fish recognition. SPIE Electronic Imaging, 2004, 5293: 157-168.
[3]Drews-Jr P, et al. Transmission estimation in underwater single images. IEEE ICCV Workshops, 2013: 825-830.
[4]Li J, et al. WaterGAN: Unsupervised generative network for real-time color correction. IEEE RA-L, 2018, 3(1): 387-394.
[5]Mildenhall B, et al. NeRF: Representing scenes as neural radiance fields. ECCV, 2020: 405-421.
[6]Ye T, Chen S, Liu Y, et al. Underwater light field retention: Neural rendering for underwater imaging[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 488-497.
[7] Kerbl B, et al. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM TOG, 2023, 42(4): 1-14.
[8]WaterNeRF: Neural Radiance Fields for Underwater Scenes
[9]WaterHE-NeRF: Water-ray Tracing Neural Radiance Fields for Underwater Scene Reconstruction
[10]Beyond NeRF Underwater: Learning Neural Reflectance Fields for True Color Correction of Marine Imagery
[11]Deborah Levy, Amit Peleg, Naama Pearl, Dan Rosenbaum, Derya Akkaynak, Simon Korman, and Tali Treibitz. Seathrunerf: Neural radiance fields in scattering media. In the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 56–65, 2023.
[12]WaterSplatting: Fast Underwater 3D Scene Reconstruction Using Gaussian Splatting
[13]SeaSplat: Representing Underwater Scenes with 3D Gaussian Splatting and a Physically Grounded Image Formation Model
[14]Aquatic-GS: A Hybrid 3D Representation for Underwater Scenes
[15]Gaussian Splashing: Direct Volumetric Rendering Underwater
[16]
